---
title: "Image Classification With Convolutional Neural Networks"
date: 2018-10-06
tags: [Classification, Deep Learning]
excerpt: "CNNs are powerful tools for image classification"
mathjax: "true"
---

# Introduction

I've heard it said that machine learning is the ideal tool for tasks that humans can perform reliably, but are difficult to explicitly program. Consider, for example, classifying hand written digits. In the image below, you can probably tell that the first row is all 0's, the second row all 1's, and so on. But imagine meeting an alien species with the same notions of number as us (they even use base 10), but have different symbols. How do you teach them what our symbols mean without writing out several examples and hoping they pick up the general pattern? How precisely do you distinguish 4 and 9, given that they have the same general shape? And, why is it that sometimes a 4 closes on top, while other times it doesn't? Why do some 7's have bars in them, and others don't? And so on.  

<img src="{{ site.url }}{{site.baseurl }}/images/mnist/mnist_example.png">

Rather than come up with specific instructions, we write a program that learns the same way we'd probably teach it: show several examples, and let the program figure out the general pattern. The quality of our image classification program will dependen on the machine learning model we use, and a class of models that work particularly well for images is convolutional neural networks, which we will abbreviate to CNN.



# Building Our CNN

We'll use Keras on top of TensorFlow to build our CNN. Keras includes functionality to download the MNIST dataset, which is the canonical dataset for testing an image classification method. The image above is a sample of some of the images in MNIST. 


```python
import os
import numpy as np
import tensorflow as tf

from keras.models import Sequential
from keras.utils import to_categorical
from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
from keras.datasets import mnist
```

    Using TensorFlow backend.


Each image in MNIST is a 28 by 28 black and white image. We'll load the data, reshape the images to 1 by 28 by 28 (the 1 meaning there's only 1 color channel), and scale the images so pixel intensity goes from 0 to 1. 


```python
(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train.reshape(X_train.shape[0],1,28,28 )
X_test = X_test.reshape(X_test.shape[0],1,28,28)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_test /= 255.0
X_train /= 255.0

input_shape = X_train.shape[1:]

num_classes = 10
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)
```

Now we'll build our model. With Keras, we first instantiate a Sequential model, and modularly add layers. We'll add some convolutional layers, a pooling layer, and then flatten the data before passing it through a single layer of a feed-forward layer, and finally we'll pass that into a softmax classifier.


```python
model = Sequential()

model.add(Conv2D(64, (3,3), data_format='channels_first', input_shape=input_shape))
model.add(Activation('relu'))

model.add(Conv2D(32,(3,3)))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Flatten())

model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(num_classes))
model.add(Activation('softmax'))
```


```python
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```


```python
model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_1 (Conv2D)            (None, 64, 26, 26)        640       
    _________________________________________________________________
    activation_1 (Activation)    (None, 64, 26, 26)        0         
    _________________________________________________________________
    conv2d_2 (Conv2D)            (None, 62, 24, 32)        7520      
    _________________________________________________________________
    activation_2 (Activation)    (None, 62, 24, 32)        0         
    _________________________________________________________________
    max_pooling2d_1 (MaxPooling2 (None, 31, 12, 32)        0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 31, 12, 32)        0         
    _________________________________________________________________
    flatten_1 (Flatten)          (None, 11904)             0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 256)               3047680   
    _________________________________________________________________
    activation_3 (Activation)    (None, 256)               0         
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 256)               0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 10)                2570      
    _________________________________________________________________
    activation_4 (Activation)    (None, 10)                0         
    =================================================================
    Total params: 3,058,410
    Trainable params: 3,058,410
    Non-trainable params: 0
    _________________________________________________________________


Now we'll fit the model to our data. This is a pretty time consuming process, taking about an hour for me to train it. It pays to save the model if you want to use it later. We can use the save method to save it as an h5 file.


```python
model.fit(X_train, y_train, epochs=10, verbose=2)
```

    Epoch 1/10
     - 508s - loss: 0.1990 - acc: 0.9392
    Epoch 2/10
     - 520s - loss: 0.0789 - acc: 0.9763
    Epoch 3/10
     - 525s - loss: 0.0575 - acc: 0.9831
    Epoch 4/10
     - 525s - loss: 0.0475 - acc: 0.9853
    Epoch 5/10
     - 500s - loss: 0.0411 - acc: 0.9869
    Epoch 6/10
     - 532s - loss: 0.0338 - acc: 0.9895
    Epoch 7/10
     - 529s - loss: 0.0304 - acc: 0.9904
    Epoch 8/10
     - 518s - loss: 0.0283 - acc: 0.9913
    Epoch 9/10
     - 513s - loss: 0.0239 - acc: 0.9924
    Epoch 10/10
     - 524s - loss: 0.0232 - acc: 0.9924





    <keras.callbacks.History at 0x1828711b70>




```python
model.save("mnist_cc_trained.h5")

from keras.models import load_model

model = load_model("mnist_cc_trained.h5")
```

To see how our model performs let's compute the accuracy for the training and test set.


```python
model_loss, model_accuracy = model.evaluate(X_test, y_test, verbose = 2)
print("Test set: Loss: {}, Accuracy: {}".format(model_loss, model_accuracy))
```

    Test set: Loss: 0.033613053735690436, Accuracy: 0.9916



```python
model_loss, model_accuracy = model.evaluate(X_train, y_train, verbose = 2)
print("Training Set: Loss: {}, Accuracy: {}".format(model_loss, model_accuracy))
```

    Training Set: Loss: 0.005310157643755414, Accuracy: 0.99815


The accuracy for both sets exceeds 99%. The model is performing quite well! We could certainly train a more sophisticated model, but it's remarkable how powerful our model is, given how little code went into it.

# Logistic Regression Model

Let's compare our sophisticared convolutional neural network with what may be the simplest classification model: logistic regression. An easy way to implement logistic regression in Keras is to construct a Sequential model with a single Dense layer, followerd by a softmax activation.


```python
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0],X_train.shape[1]**2 )
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1]**2 )
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_test /= 255.0
X_train /= 255.0

input_shape = X_train.shape[1:]

num_classes = 10
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)
```


```python
simple_model = Sequential()

simple_model.add(Dense(num_classes,input_shape=input_shape))
simple_model.add(Activation('softmax'))
simple_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
```


```python
simple_model.summary()
```

    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense_10 (Dense)             (None, 10)                7850      
    _________________________________________________________________
    activation_11 (Activation)   (None, 10)                0         
    =================================================================
    Total params: 7,850
    Trainable params: 7,850
    Non-trainable params: 0
    _________________________________________________________________



```python
simple_model.fit(X_train, y_train, epochs=50, verbose=0)
```




    <keras.callbacks.History at 0x1819b8ac88>




```python
simple_model_loss, simple_model_accuracy = simple_model.evaluate(X_test, y_test, verbose = 0)
print("Test set: Loss: {}, Accuracy: {}".format(simple_model_loss, simple_model_accuracy))
```

    Test set: Loss: 0.2720457934588194, Accuracy: 0.9231



```python
simple_model_loss, simple_model_accuracy = simple_model.evaluate(X_train, y_train, verbose = 0)
print("Train set: Loss: {}, Accuracy: {}".format(simple_model_loss, simple_model_accuracy))
```

    Train set: Loss: 0.2696952307711045, Accuracy: 0.9257


The accuracy of our logistic regression model is a little over 92%. While certainly not as good as the 99% we got with the CNN, this model trained in just a couple of minutes.
