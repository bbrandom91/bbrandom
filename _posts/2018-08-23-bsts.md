---
title: "Using Bayesian Time Series For Forecasting"
date: 2018-08-23
tags: [time series, bsts]
excerpt: "A Bayesian Perspective On Time Seies"
mathjax: "true"
---

# Introduction

Time series data can be tricky to work with for a couple of reasons. First, each point in the time series (by a point I mean a time-value and observation-value pair) is correlated with other points in the past, a phenomenon called autocorrelation. Second, a time series represents a single realization of a stochastic process, and we generally only have a single time series. In this sense a time series is like a single data point. Compare to this to, say, a case where we would use linear regression ( or something similar), where we have multiple, independent data points. This new perspective poses some difficulties since frequentist methods like maximum likelihood estimation of parameters and confidence intervals which rely on having lots of data don't make a ton of sense. 

Bayesian models tackle this problem by placing the burden of modelling uncertainty on the user. The way we do this is by representing uncertainty with a prior distribution on the parameters, and it is up to the user to choose a prior that refelcts their uncertainty, including complete uncertainty. Once we have actual data we get a so-called posterior distribution on the parameters that reflect our updated knowledge. 

With these principles in mind let's look at a time series and model it using a so-called Bayesian Structural Time Series (BSTS). BSTS models are nice for a few reasons:

1) Uncertainty is taken into account for future predictions using so-called credible intervals
2) It's easy to understand how BSTS models work because they are linear
3) Additional predictors can be incorporated by adding a regression component to the model

The general form of a BSTS model is a sum of a few components: Response = Trend + Seasonality + Regression + Error. There are different ways to model each component, leading to a lot of flexibility. 

Alright, enough talk, let's write some code.

# BSTS: First Pass

We'll be using R for the remainder of this post. First we load up some libraries, the most important one for our purposes being bsts. 

```r
library(lubridate)
library(bsts)
library(dplyr)
library(ggplot2)
library(ggfortify) 
library(forecast)
```

The data we will be using is quarterly earning in dollars per Johnson & Johnson share between the years 1960 and 1980. 

```r
data("JohnsonJohnson")
jj <- JohnsonJohnson

autoplot(jj)+
  ggtitle('Johnson And Johnson Earnings')+
  xlab('Year')+
  ylab('Earnings')
ggsave("jj1.png")
```

<img src="{{ site.url }}{{site.baseurl }}/images/abalone/jj1.png">

The trend is clearly nonlinear, and the fluctuations are growing over time. The error term in the BSTS formula assumes constant variance at all time, and in situations like this a common remedy is to take a log of the data. I've found instead using an arcsinh transform accomplishes the same thing and has some additional quality of life improvements that I won't go into right now. 

```r
autoplot(asinh(jj))+
  ggtitle('Johnson And Johnson Earnings')+
  xlab('Year')+
  ylab('arcsinh(Earnings)')
ggsave("jj2.png")
```

<img src="{{ site.url }}{{site.baseurl }}/images/abalone/jj2.png">

The trend looks much more linear now and the time-dependent variance seems to be under control. 

Let's get to modeling. We'll create a train-test split where the test data is everything after the second quarter of 1979. Since there are no predictors there is no regression component to the model. To test the model we will use mean absolute percent error (MAPE) on the test data, and we'll also plot a 95% credible interval for the prediction.

```r

# Model setup
Y.train <- window(jj, start(jj), end=c(1979,2) ) 
Y.test <- window(jj, start=c(1979,3))
y.train <- asinh(0.5*Y.train)
y.test <- asinh(0.5*Y.test)
  
# Forecast length
  
forecast.horizon <- length(window(jj, start=c(1979,3), end=c(1980,4)))
  
  
# Add model components and instantiate a bsts model
model_components <- list()
model_components <- AddSemilocalLinearTrend(model_components,y.train)
model_components <- AddSeasonal(model_components, y.train, nseasons=4)
#model_components <- AddTrig(model_components, y.train, period=4, frequencies=1:2)
bsts.model <- bsts(y.train, state.specification = model_components, niter = 500, ping=0, seed=137)

# Get a suggested number of burn-ins
burn <- SuggestBurn(0.1, bsts.model)

# Predict future values with confidence interval
pred <- predict.bsts(bsts.model, horizon = forecast.horizon, burn = burn, quantiles = c(.025, .975))
  
# Actual versus predicted
d2 <- data.frame(
 #fitted values and predictions
  c(2*sinh(as.numeric(-colMeans(bsts.model$one.step.prediction.errors[-(1:burn),])+y.train)),  
  2*sinh(as.numeric(pred$mean))),
  # actual data and dates 
  as.numeric(jj),
  as.Date(time(jj)))
names(d2) <- c("Fitted", "Actual", "Date")
  
### MAPE
MAPE <- filter(d2, Date>as.Date("1979-06-06") ) %>% summarise(MAPE=mean(abs((Actual-Fitted)/Actual)))

### 95% forecast credible interval
posterior.interval <- cbind.data.frame(
2*sinh(as.numeric(pred$interval[1,])),
2*sinh(as.numeric(pred$interval[2,])), 
subset(d2, Date>as.Date("1979-06-06") )$Date)
names(posterior.interval) <- c("LL", "UL", "Date")

### Join intervals to the forecast
d3 <- left_join(d2, posterior.interval, by="Date")
  
### Plot
bsts.plot <- ggplot(data=d3, aes(x=Date)) +
  geom_line(aes(y=Actual, colour = "Actual"), size=1.2) +
  geom_line(aes(y=Fitted, colour = "Fitted"), size=1.2, linetype=1) +
  theme_bw() + theme(legend.title = element_blank()) + ylab("") + xlab("") +
  geom_vline(xintercept=as.numeric(as.Date("1979-06-01")), linetype=2) + 
  geom_ribbon(aes(ymin=LL, ymax=UL), fill="grey", alpha=0.5) +
  ggtitle(paste0("BSTS -- Holdout MAPE = ", round(100*MAPE,2), "%")) +
  theme(axis.text.x=element_text(angle = -90, hjust = 0))

bsts.plot

ggsave("jj3.png")

```

<img src="{{ site.url }}{{site.baseurl }}/images/abalone/jj3.png">

The model seems to be performing pretty well! It captures the trend very well and does a servicable job with the seasonality. Something really cool about the bsts package is that we can visualize the individual model components.

```r
### Extract the components
library(reshape2)
components <- cbind.data.frame(
  colMeans(bsts.model$state.contributions[-(1:burn),"trend",]),                               
  colMeans(bsts.model$state.contributions[-(1:burn),"seasonal.4.1",]),
  as.Date(time(y.train)))  
names(components) <- c("Trend", "Seasonality", "Date")
components <- melt(components, id="Date")
names(components) <- c("Date", "Component", "Value")

### Plot
ggplot(data=components, aes(x=Date, y=2*sinh(Value) )) + geom_line() + 
  theme_bw() + theme(legend.title = element_blank()) + ylab("") + xlab("") + 
  facet_grid(Component ~ ., scales="free") + guides(colour=FALSE) + 
  theme(axis.text.x=element_text(angle = -90, hjust = 0))

ggsave("jj4.png")
```

<img src="{{ site.url }}{{site.baseurl }}/images/abalone/jj4.png">

# Adding Regression Component

The data we just looked at doesn't have any additional predictors other than time. It's a testament to the power of our model that we got good results without any regressors. Still, bsts allows us to add additional regressors quite easily, and there are some interesting things to say about the regression coefficients.

Our last dataset didn't have any predictors, so let's get another dataset. We'll be looking at a dataset from bsts called new.home.sales. The values are (the z-score of) home sales in the US, while the predictors are search terms from Google trends.

```r
data(new.home.sales)

```

Let's take a look at the main series we wish to model:

```r
autoplot( new.home.sales$HSN1FNSA)+
    ggtitle('Home Sales In US')+
  xlab('Year')+
  ylab('Z-score of Sales')

ggsave("homesales1.png")
```

<img src="{{ site.url }}{{site.baseurl }}/images/abalone/homesales1.png">

Let's go ahead and fit a model. We'll assume an annual season, and we won't worry about the error and credible intervals.

```r

### Fit the model with regressors
model_components <- list()
model_components <- AddLocalLinearTrend(model_components, new.home.sales$HSN1FNSA)
model_components <- AddSeasonal(model_components, new.home.sales$HSN1FNSA, nseasons = 12)
bsts.reg <- bsts(HSN1FNSA ~ ., state.specification = model_components, data =
                new.home.sales, niter = 500, ping=0, seed=137)

### Get the number of burn-ins to discard
burn <- SuggestBurn(0.1, bsts.reg)

# Actual versus predicted
d2.reg <- data.frame(
 #fitted values and predictions
  c(as.numeric(-colMeans(bsts.reg$one.step.prediction.errors[-(1:burn),])+new.home.sales$HSN1FNSA)),  
  # actual data and dates 
  as.numeric(new.home.sales$HSN1FNSA),
  as.Date(time(new.home.sales$HSN1FNSA)))
names(d2.reg) <- c("Fitted", "Actual", "Date")


### Plot
bsts.reg.plot <- ggplot(data=d2.reg, aes(x=Date)) +
  geom_line(aes(y=Actual, colour = "Actual"), size=1.2) +
  geom_line(aes(y=Fitted, colour = "Fitted"), size=1.2, linetype=1) +
  theme_bw() + theme(legend.title = element_blank()) + ylab("Z-Score of Sales") + xlab("Year") +
  theme(axis.text.x=element_text(angle = -90, hjust = 0))

bsts.reg.plot

ggsave("homesales2.png")

```

<img src="{{ site.url }}{{site.baseurl }}/images/abalone/homesales2.png">

Pretty decent if I do say so myself. It doesn't perfectly capture all the fine features, but it's amazing how well it follows changes in trends over roughly a half-year basis. For some reason it does worst at the very beginning; I wonder why.

Something neat about BSTS models is that regression coefficients are modelled with a spike-and-slab prior. Without going too much detail, this means that whenever we evaluate the time series at a time value, we flip a biased coin for each regression coefficient (a different coin for each coefficient), and we only use those coefficients whose coin toss landed heads. The most important regression coefficients are those for which the respective coin is highy biased towards landing heads.

Let's look at the mean value for each regression coefficient as well as the probability of using them in a model:

```r
### Helper function to get the positive mean of a vector
PositiveMean <- function(b) {
  b <- b[abs(b) > 0]
  if (length(b) > 0) 
    return(mean(b))
  return(0)
}

### Get the average coefficients when variables were selected (non-zero slopes)
coeff <- data.frame(melt(apply(bsts.reg$coefficients[-(1:burn),], 2, PositiveMean)))
coeff$Variable <- as.character(row.names(coeff))
ggplot(data=coeff, aes(x=Variable, y=value)) + 
  geom_bar(stat="identity", position="identity") + 
  theme(axis.text.x=element_text(angle = -90, hjust = 0)) +
  xlab("") + ylab("") + ggtitle("Average coefficients")

ggsave("homesales3.png")
```

<img src="{{ site.url }}{{site.baseurl }}/images/abalone/homesales3.png">

```r

### Inclusion probabilities -- i.e., how often were the variables selected 
inclusionprobs <- melt(colMeans(bsts.reg$coefficients[-(1:burn),] != 0))
inclusionprobs$Variable <- as.character(row.names(inclusionprobs))
ggplot(data=inclusionprobs, aes(x=Variable, y=value)) + 
  geom_bar(stat="identity", position="identity") + 
  theme(axis.text.x=element_text(angle = -90, hjust = 0)) + 
  xlab("") + ylab("") + ggtitle("Inclusion probabilities")

ggsave("homesales4.png")
```

<img src="{{ site.url }}{{site.baseurl }}/images/abalone/homesales4.png">

The variable appreciation.rates has a nearly 100% selection, with nothing else coming even close to that.

As before, we can plot the trend, seasonality, and regression components separately.

```r
### Get the components
components.withreg <- cbind.data.frame(
  colMeans(bsts.reg$state.contributions[-(1:burn),"trend",]),
  colMeans(bsts.reg$state.contributions[-(1:burn),"seasonal.12.1",]),
  colMeans(bsts.reg$state.contributions[-(1:burn),"regression",]),
  as.Date(time(new.home.sales)))  
names(components.withreg) <- c("Trend", "Seasonality", "Regression", "Date")
components.withreg <- melt(components.withreg, id.vars="Date")
names(components.withreg) <- c("Date", "Component", "Value")

ggplot(data=components.withreg, aes(x=Date, y=Value)) + geom_line() + 
  theme_bw() + theme(legend.title = element_blank()) + ylab("") + xlab("") + 
  facet_grid(Component ~ ., scales="free") + guides(colour=FALSE) + 
  theme(axis.text.x=element_text(angle = -90, hjust = 0))

ggsave("homesales5.png")
```

<img src="{{ site.url }}{{site.baseurl }}/images/abalone/homesales5.png">

# Conclusion

Bayesian structural time series are awesome models for time series because they are easy to implement, simple to understand, and quite powerful. The most popular time series models, ARIMA, have comparable performance, but are more restrictive in their assumptions and don't handle uncertainty as elegantly as BSTS. Something I would have liked to talk about some more is that the user can specify their own prior on the regression coefficients, such as manually setting some of the inclusion probabilities to zero or one. This can be useful if the user has a good idea of what are the most important predictors to use.

I would like to write another post about deep learning approaches to time series (GRU and LSTM). While BSTS is nice for producing a model you can understand, it's hard to deny the raw power of RNNs, and I'm curious to see how they perform.  